"""
OOF (Out-of-Fold) ì˜ˆì¸¡ ìƒì„± ìŠ¤í¬ë¦½íŠ¸

3ê°œ ëª¨ë¸(XGBoost, LightGBM, CatBoost)ì˜ OOF ì˜ˆì¸¡ì„ ìƒì„±í•©ë‹ˆë‹¤.
Stacking ì•™ìƒë¸”ì˜ Meta-Learner í•™ìŠµì— ì‚¬ìš©ë©ë‹ˆë‹¤.

"""

import pandas as pd
import numpy as np
import pickle
from pathlib import Path
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
from tqdm import tqdm

# ê²½ë¡œ ì„¤ì •
DATA_DIR = Path('data/processed')
MODEL_DIR = Path('models')
OUTPUT_DIR = Path('data/processed')

print("=" * 60)
print("OOF ì˜ˆì¸¡ ìƒì„± ì‹œì‘")
print("=" * 60)

# 1. ë°ì´í„° ë¡œë“œ
print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
df = pd.read_csv(DATA_DIR / 'train_final_passes_v6.csv')
print(f"âœ… Shape: {df.shape}")

# 2. í”¼ì²˜ ë° íƒ€ê²Ÿ ë¶„ë¦¬
target_cols = ['end_x', 'end_y']
feature_cols = [col for col in df.columns if col not in target_cols + ['game_episode']]

print(f"\nğŸ” ë²”ì£¼í˜• í”¼ì²˜ í™•ì¸ ì¤‘...")

# ë²”ì£¼í˜• í”¼ì²˜ ì¸ì½”ë”©
categorical_cols = df[feature_cols].select_dtypes(include=['object']).columns.tolist()
if len(categorical_cols) > 0:
    print(f"ğŸ“ ë²”ì£¼í˜• í”¼ì²˜ ë°œê²¬: {categorical_cols}")
    from sklearn.preprocessing import LabelEncoder
    
    for col in categorical_cols:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col].astype(str))
    print(f"âœ… ë²”ì£¼í˜• í”¼ì²˜ ì¸ì½”ë”© ì™„ë£Œ")
else:
    print(f"âœ… ë²”ì£¼í˜• í”¼ì²˜ ì—†ìŒ")

X = df[feature_cols].values
y_x = df['end_x'].values
y_y = df['end_y'].values

print(f"âœ… í”¼ì²˜: {len(feature_cols)}ê°œ")
print(f"âœ… ìƒ˜í”Œ: {len(X):,}ê°œ")

# 3. K-Fold ì„¤ì •
N_SPLITS = 5
kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)

print(f"\nğŸ”€ {N_SPLITS}-Fold Cross-Validation")

# 4. OOF ì˜ˆì¸¡ ì €ì¥ ë°°ì—´ ì´ˆê¸°í™”
oof_predictions = {
    'xgb': {'x': np.zeros(len(X)), 'y': np.zeros(len(X))},
    'lgb': {'x': np.zeros(len(X)), 'y': np.zeros(len(X))},
    'cat': {'x': np.zeros(len(X)), 'y': np.zeros(len(X))}
}

# 5. ê° ëª¨ë¸ë³„ OOF ìƒì„±
print("\n" + "=" * 60)
print("1ï¸âƒ£ XGBoost OOF ìƒì„±")
print("=" * 60)

for fold, (train_idx, val_idx) in enumerate(tqdm(kf.split(X), total=N_SPLITS, desc="XGBoost")):
    # Train/Val ë¶„ë¦¬
    X_train, X_val = X[train_idx], X[val_idx]
    y_train_x, y_val_x = y_x[train_idx], y_x[val_idx]
    y_train_y, y_val_y = y_y[train_idx], y_y[val_idx]
    
    # XGBoost íŒŒë¼ë¯¸í„° (v4 íŠœë‹ëœ ë²„ì „)
    params = {
        'objective': 'reg:squarederror',
        'max_depth': 8,
        'learning_rate': 0.05,
        'n_estimators': 300,
        'random_state': 42
    }
    
    # end_x ì˜ˆì¸¡
    model_x = xgb.XGBRegressor(**params)
    model_x.fit(X_train, y_train_x, verbose=False)
    oof_predictions['xgb']['x'][val_idx] = model_x.predict(X_val)
    
    # end_y ì˜ˆì¸¡
    model_y = xgb.XGBRegressor(**params)
    model_y.fit(X_train, y_train_y, verbose=False)
    oof_predictions['xgb']['y'][val_idx] = model_y.predict(X_val)

# XGBoost OOF RMSE ê³„ì‚°
xgb_rmse_x = np.sqrt(mean_squared_error(y_x, oof_predictions['xgb']['x']))
xgb_rmse_y = np.sqrt(mean_squared_error(y_y, oof_predictions['xgb']['y']))
xgb_rmse = np.sqrt((xgb_rmse_x**2 + xgb_rmse_y**2) / 2)
print(f"\nâœ… XGBoost OOF RMSE: {xgb_rmse:.4f}m")
print(f"   - end_x RMSE: {xgb_rmse_x:.4f}m")
print(f"   - end_y RMSE: {xgb_rmse_y:.4f}m")

# -------------------------------------------------------------------
print("\n" + "=" * 60)
print("2ï¸âƒ£ LightGBM OOF ìƒì„±")
print("=" * 60)

for fold, (train_idx, val_idx) in enumerate(tqdm(kf.split(X), total=N_SPLITS, desc="LightGBM")):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train_x, y_val_x = y_x[train_idx], y_x[val_idx]
    y_train_y, y_val_y = y_y[train_idx], y_y[val_idx]
    
    # LightGBM íŒŒë¼ë¯¸í„° (v4 íŠœë‹ëœ ë²„ì „)
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 63,
        'learning_rate': 0.03,
        'feature_fraction': 0.7,
        'bagging_fraction': 0.7,
        'bagging_freq': 5,
        'min_child_samples': 20,
        'reg_alpha': 0.1,
        'reg_lambda': 0.1,
        'verbosity': -1,
        'random_state': 42
    }
    
    # end_x ì˜ˆì¸¡
    train_data_x = lgb.Dataset(X_train, label=y_train_x)
    model_x = lgb.train(params, train_data_x, num_boost_round=800)
    oof_predictions['lgb']['x'][val_idx] = model_x.predict(X_val)
    
    # end_y ì˜ˆì¸¡
    train_data_y = lgb.Dataset(X_train, label=y_train_y)
    model_y = lgb.train(params, train_data_y, num_boost_round=800)
    oof_predictions['lgb']['y'][val_idx] = model_y.predict(X_val)

# LightGBM OOF RMSE ê³„ì‚°
lgb_rmse_x = np.sqrt(mean_squared_error(y_x, oof_predictions['lgb']['x']))
lgb_rmse_y = np.sqrt(mean_squared_error(y_y, oof_predictions['lgb']['y']))
lgb_rmse = np.sqrt((lgb_rmse_x**2 + lgb_rmse_y**2) / 2)
print(f"\nâœ… LightGBM OOF RMSE: {lgb_rmse:.4f}m")
print(f"   - end_x RMSE: {lgb_rmse_x:.4f}m")
print(f"   - end_y RMSE: {lgb_rmse_y:.4f}m")

# -------------------------------------------------------------------
print("\n" + "=" * 60)
print("3ï¸âƒ£ CatBoost OOF ìƒì„±")
print("=" * 60)

for fold, (train_idx, val_idx) in enumerate(tqdm(kf.split(X), total=N_SPLITS, desc="CatBoost")):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train_x, y_val_x = y_x[train_idx], y_x[val_idx]
    y_train_y, y_val_y = y_y[train_idx], y_y[val_idx]
    
    # CatBoost íŒŒë¼ë¯¸í„° (v4)
    params = {
        'iterations': 500,
        'depth': 8,
        'learning_rate': 0.05,
        'loss_function': 'RMSE',
        'random_seed': 42,
        'verbose': False
    }
    
    # end_x ì˜ˆì¸¡
    model_x = cb.CatBoostRegressor(**params)
    model_x.fit(X_train, y_train_x, verbose=False)
    oof_predictions['cat']['x'][val_idx] = model_x.predict(X_val)
    
    # end_y ì˜ˆì¸¡
    model_y = cb.CatBoostRegressor(**params)
    model_y.fit(X_train, y_train_y, verbose=False)
    oof_predictions['cat']['y'][val_idx] = model_y.predict(X_val)

# CatBoost OOF RMSE ê³„ì‚°
cat_rmse_x = np.sqrt(mean_squared_error(y_x, oof_predictions['cat']['x']))
cat_rmse_y = np.sqrt(mean_squared_error(y_y, oof_predictions['cat']['y']))
cat_rmse = np.sqrt((cat_rmse_x**2 + cat_rmse_y**2) / 2)
print(f"\nâœ… CatBoost OOF RMSE: {cat_rmse:.4f}m")
print(f"   - end_x RMSE: {cat_rmse_x:.4f}m")
print(f"   - end_y RMSE: {cat_rmse_y:.4f}m")

# -------------------------------------------------------------------
print("\n" + "=" * 60)
print("ğŸ“Š OOF ì„±ëŠ¥ ìš”ì•½")
print("=" * 60)
print(f"XGBoost : {xgb_rmse:.4f}m")
print(f"LightGBM: {lgb_rmse:.4f}m")
print(f"CatBoost: {cat_rmse:.4f}m")
print(f"í‰ê·     : {(xgb_rmse + lgb_rmse + cat_rmse) / 3:.4f}m")

# -------------------------------------------------------------------
print("\n" + "=" * 60)
print("ğŸ’¾ OOF ì˜ˆì¸¡ ì €ì¥")
print("=" * 60)

# OOF DataFrame ìƒì„±
oof_df = pd.DataFrame({
    'game_episode': df['game_episode'],
    'true_x': y_x,
    'true_y': y_y,
    'xgb_pred_x': oof_predictions['xgb']['x'],
    'xgb_pred_y': oof_predictions['xgb']['y'],
    'lgb_pred_x': oof_predictions['lgb']['x'],
    'lgb_pred_y': oof_predictions['lgb']['y'],
    'cat_pred_x': oof_predictions['cat']['x'],
    'cat_pred_y': oof_predictions['cat']['y']
})

# ì €ì¥
output_path = OUTPUT_DIR / 'oof_predictions_v6.csv'
oof_df.to_csv(output_path, index=False)
print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")
print(f"   Shape: {oof_df.shape}")

# -------------------------------------------------------------------
print("\n" + "=" * 60)
print("ğŸ‰ OOF ìƒì„± ì™„ë£Œ!")
print("=" * 60)
print("\në‹¤ìŒ ë‹¨ê³„:")
print("1. Meta-Learner í•™ìŠµ: python src/models/train_meta_learner.py")
print("2. Stacking ì˜ˆì¸¡: python src/models/predict_stacking.py")
