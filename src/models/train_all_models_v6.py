"""
Phase 6 ëª¨ë¸ í•™ìŠµ: 3ê°œ ëª¨ë¸ í†µí•© í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

ì—ëŸ¬ ë¶„ì„ ê¸°ë°˜ íƒ€ê²ŸíŒ… í”¼ì²˜ í¬í•¨
- XGBoost
- LightGBM  
- CatBoost
"""

import pandas as pd
import numpy as np
import pickle
from pathlib import Path
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
from tqdm import tqdm

# ê²½ë¡œ ì„¤ì •
DATA_DIR = Path('data/processed')
MODEL_DIR = Path('models')
MODEL_DIR.mkdir(exist_ok=True)

print("=" * 60)
print("Phase 6: 3ê°œ ëª¨ë¸ í†µí•© í•™ìŠµ")
print("=" * 60)

# =================================================================
# 1. ë°ì´í„° ë¡œë“œ
# =================================================================
print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
df = pd.read_csv(DATA_DIR / 'train_final_passes_v6.csv')
print(f"âœ… Shape: {df.shape}")

# =================================================================
# 2. í”¼ì²˜ ë° íƒ€ê²Ÿ ë¶„ë¦¬
# =================================================================
target_cols = ['end_x', 'end_y']
feature_cols = [col for col in df.columns if col not in target_cols + ['game_episode']]

print(f"\nğŸ” í”¼ì²˜ í™•ì¸...")
print(f"   ì´ í”¼ì²˜: {len(feature_cols)}ê°œ")

# ë²”ì£¼í˜• í”¼ì²˜ ì¸ì½”ë”©
categorical_cols = df[feature_cols].select_dtypes(include=['object']).columns.tolist()
if len(categorical_cols) > 0:
    print(f"   ë²”ì£¼í˜• í”¼ì²˜: {len(categorical_cols)}ê°œ")
    from sklearn.preprocessing import LabelEncoder
    
    label_encoders = {}
    for col in categorical_cols:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col].astype(str))
        label_encoders[col] = le
    
    # LabelEncoder ì €ì¥
    with open(MODEL_DIR / 'label_encoders_v6.pkl', 'wb') as f:
        pickle.dump(label_encoders, f)
    print(f"   âœ… ë²”ì£¼í˜• ì¸ì½”ë”© ì™„ë£Œ")

X = df[feature_cols].values
y_x = df['end_x'].values
y_y = df['end_y'].values

print(f"âœ… X shape: {X.shape}")
print(f"âœ… y_x shape: {y_x.shape}")
print(f"âœ… y_y shape: {y_y.shape}")

# =================================================================
# 3. Cross-Validation ì„¤ì •
# =================================================================
N_SPLITS = 5
kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)

print(f"\nğŸ”€ {N_SPLITS}-Fold Cross-Validation")

# =================================================================
# 4. XGBoost í•™ìŠµ
# =================================================================
print("\n" + "=" * 60)
print("1ï¸âƒ£ XGBoost í•™ìŠµ")
print("=" * 60)

xgb_params = {
    'objective': 'reg:squarederror',
    'max_depth': 8,
    'learning_rate': 0.05,
    'n_estimators': 300,
    'random_state': 42
}

cv_scores_xgb = []

for fold, (train_idx, val_idx) in enumerate(tqdm(kf.split(X), total=N_SPLITS, desc="XGBoost CV")):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train_x, y_val_x = y_x[train_idx], y_x[val_idx]
    y_train_y, y_val_y = y_y[train_idx], y_y[val_idx]
    
    # end_x ëª¨ë¸
    model_x = xgb.XGBRegressor(**xgb_params)
    model_x.fit(X_train, y_train_x, verbose=False)
    pred_x = model_x.predict(X_val)
    
    # end_y ëª¨ë¸
    model_y = xgb.XGBRegressor(**xgb_params)
    model_y.fit(X_train, y_train_y, verbose=False)
    pred_y = model_y.predict(X_val)
    
    # RMSE ê³„ì‚°
    rmse_x = np.sqrt(mean_squared_error(y_val_x, pred_x))
    rmse_y = np.sqrt(mean_squared_error(y_val_y, pred_y))
    rmse = np.sqrt((rmse_x**2 + rmse_y**2) / 2)
    
    cv_scores_xgb.append(rmse)

xgb_cv_mean = np.mean(cv_scores_xgb)
xgb_cv_std = np.std(cv_scores_xgb)

print(f"\nâœ… XGBoost CV ê²°ê³¼:")
print(f"   í‰ê·  RMSE: {xgb_cv_mean:.4f}m (Â±{xgb_cv_std:.4f})")
print(f"   Foldë³„: {[f'{s:.4f}' for s in cv_scores_xgb]}")

# ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… í•™ìŠµ
print(f"\nğŸ”„ ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… í•™ìŠµ ì¤‘...")
xgb_model_x = xgb.XGBRegressor(**xgb_params)
xgb_model_x.fit(X, y_x, verbose=False)

xgb_model_y = xgb.XGBRegressor(**xgb_params)
xgb_model_y.fit(X, y_y, verbose=False)

# ì €ì¥
with open(MODEL_DIR / 'baseline_model_v6.pkl', 'wb') as f:
    pickle.dump({'model_x': xgb_model_x, 'model_y': xgb_model_y}, f)

print(f"âœ… ì €ì¥: baseline_model_v6.pkl")

# =================================================================
# 5. LightGBM í•™ìŠµ
# =================================================================
print("\n" + "=" * 60)
print("2ï¸âƒ£ LightGBM í•™ìŠµ")
print("=" * 60)

lgb_params = {
    'objective': 'regression',
    'metric': 'rmse',
    'boosting_type': 'gbdt',
    'num_leaves': 63,
    'learning_rate': 0.03,
    'feature_fraction': 0.7,
    'bagging_fraction': 0.7,
    'bagging_freq': 5,
    'min_child_samples': 20,
    'reg_alpha': 0.1,
    'reg_lambda': 0.1,
    'verbosity': -1,
    'random_state': 42
}

cv_scores_lgb = []

for fold, (train_idx, val_idx) in enumerate(tqdm(kf.split(X), total=N_SPLITS, desc="LightGBM CV")):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train_x, y_val_x = y_x[train_idx], y_x[val_idx]
    y_train_y, y_val_y = y_y[train_idx], y_y[val_idx]
    
    # end_x ëª¨ë¸
    train_data_x = lgb.Dataset(X_train, label=y_train_x)
    model_x = lgb.train(lgb_params, train_data_x, num_boost_round=800)
    pred_x = model_x.predict(X_val)
    
    # end_y ëª¨ë¸
    train_data_y = lgb.Dataset(X_train, label=y_train_y)
    model_y = lgb.train(lgb_params, train_data_y, num_boost_round=800)
    pred_y = model_y.predict(X_val)
    
    # RMSE ê³„ì‚°
    rmse_x = np.sqrt(mean_squared_error(y_val_x, pred_x))
    rmse_y = np.sqrt(mean_squared_error(y_val_y, pred_y))
    rmse = np.sqrt((rmse_x**2 + rmse_y**2) / 2)
    
    cv_scores_lgb.append(rmse)

lgb_cv_mean = np.mean(cv_scores_lgb)
lgb_cv_std = np.std(cv_scores_lgb)

print(f"\nâœ… LightGBM CV ê²°ê³¼:")
print(f"   í‰ê·  RMSE: {lgb_cv_mean:.4f}m (Â±{lgb_cv_std:.4f})")
print(f"   Foldë³„: {[f'{s:.4f}' for s in cv_scores_lgb]}")

# ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… í•™ìŠµ
print(f"\nğŸ”„ ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… í•™ìŠµ ì¤‘...")
train_data_x = lgb.Dataset(X, label=y_x)
lgb_model_x = lgb.train(lgb_params, train_data_x, num_boost_round=800)

train_data_y = lgb.Dataset(X, label=y_y)
lgb_model_y = lgb.train(lgb_params, train_data_y, num_boost_round=800)

# ì €ì¥
with open(MODEL_DIR / 'lgb_model_v6.pkl', 'wb') as f:
    pickle.dump({'model_x': lgb_model_x, 'model_y': lgb_model_y}, f)

print(f"âœ… ì €ì¥: lgb_model_v6.pkl")

# =================================================================
# 6. CatBoost í•™ìŠµ
# =================================================================
print("\n" + "=" * 60)
print("3ï¸âƒ£ CatBoost í•™ìŠµ")
print("=" * 60)

cat_params = {
    'iterations': 500,
    'depth': 8,
    'learning_rate': 0.05,
    'loss_function': 'RMSE',
    'random_seed': 42,
    'verbose': False
}

cv_scores_cat = []

for fold, (train_idx, val_idx) in enumerate(tqdm(kf.split(X), total=N_SPLITS, desc="CatBoost CV")):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train_x, y_val_x = y_x[train_idx], y_x[val_idx]
    y_train_y, y_val_y = y_y[train_idx], y_y[val_idx]
    
    # end_x ëª¨ë¸
    model_x = cb.CatBoostRegressor(**cat_params)
    model_x.fit(X_train, y_train_x, verbose=False)
    pred_x = model_x.predict(X_val)
    
    # end_y ëª¨ë¸
    model_y = cb.CatBoostRegressor(**cat_params)
    model_y.fit(X_train, y_train_y, verbose=False)
    pred_y = model_y.predict(X_val)
    
    # RMSE ê³„ì‚°
    rmse_x = np.sqrt(mean_squared_error(y_val_x, pred_x))
    rmse_y = np.sqrt(mean_squared_error(y_val_y, pred_y))
    rmse = np.sqrt((rmse_x**2 + rmse_y**2) / 2)
    
    cv_scores_cat.append(rmse)

cat_cv_mean = np.mean(cv_scores_cat)
cat_cv_std = np.std(cv_scores_cat)

print(f"\nâœ… CatBoost CV ê²°ê³¼:")
print(f"   í‰ê·  RMSE: {cat_cv_mean:.4f}m (Â±{cat_cv_std:.4f})")
print(f"   Foldë³„: {[f'{s:.4f}' for s in cv_scores_cat]}")

# ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… í•™ìŠµ
print(f"\nğŸ”„ ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… í•™ìŠµ ì¤‘...")
cat_model_x = cb.CatBoostRegressor(**cat_params)
cat_model_x.fit(X, y_x, verbose=False)

cat_model_y = cb.CatBoostRegressor(**cat_params)
cat_model_y.fit(X, y_y, verbose=False)

# ì €ì¥
with open(MODEL_DIR / 'catboost_model_v6.pkl', 'wb') as f:
    pickle.dump({'model_x': cat_model_x, 'model_y': cat_model_y}, f)

print(f"âœ… ì €ì¥: catboost_model_v6.pkl")

# =================================================================
# 7. ìµœì¢… ê²°ê³¼ ìš”ì•½
# =================================================================
print("\n" + "=" * 60)
print("ğŸ“Š ìµœì¢… CV ê²°ê³¼ ìš”ì•½")
print("=" * 60)

print(f"\n{'ëª¨ë¸':<15} {'v4 CV':>10} {'v6 CV':>10} {'ê°œì„ ':>10}")
print("-" * 50)

# v4 ê²°ê³¼ (í•˜ë“œì½”ë”©)
v4_xgb = 18.73
v4_lgb = 18.64
v4_cat = 18.73

print(f"{'XGBoost':<15} {v4_xgb:>9.2f}m {xgb_cv_mean:>9.2f}m {xgb_cv_mean - v4_xgb:>+9.2f}m")
print(f"{'LightGBM':<15} {v4_lgb:>9.2f}m {lgb_cv_mean:>9.2f}m {lgb_cv_mean - v4_lgb:>+9.2f}m")
print(f"{'CatBoost':<15} {v4_cat:>9.2f}m {cat_cv_mean:>9.2f}m {cat_cv_mean - v4_cat:>+9.2f}m")

v4_avg = (v4_xgb + v4_lgb + v4_cat) / 3
v6_avg = (xgb_cv_mean + lgb_cv_mean + cat_cv_mean) / 3

print("-" * 50)
print(f"{'í‰ê· ':<15} {v4_avg:>9.2f}m {v6_avg:>9.2f}m {v6_avg - v4_avg:>+9.2f}m")

# ê°œì„ ìœ¨ ê³„ì‚°
improvement = ((v4_avg - v6_avg) / v4_avg) * 100

print(f"\nâœ¨ ì´ ê°œì„ : {improvement:+.2f}%")

if v6_avg < v4_avg:
    print(f"ğŸ‰ Phase 6 í”¼ì²˜ê°€ íš¨ê³¼ì ì…ë‹ˆë‹¤!")
else:
    print(f"âš ï¸ Phase 6 í”¼ì²˜ íš¨ê³¼ê°€ ë¯¸ë¯¸í•˜ê±°ë‚˜ ì•…í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"   â†’ í”¼ì²˜ ì„ íƒ ë˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš”")

print("\n" + "=" * 60)
print("âœ… ëª¨ë“  ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
print("=" * 60)

print("\në‹¤ìŒ ë‹¨ê³„:")
print("  1. OOF ìƒì„±: python src/models/generate_oof_predictions_v6.py")
print("  2. Stacking: python src/models/predict_stacking_v6.py")
print("  3. ì œì¶œ!")
