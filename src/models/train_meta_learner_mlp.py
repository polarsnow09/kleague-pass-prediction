"""
MLP Meta-Learner í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

OOF ì˜ˆì¸¡ì„ ì‚¬ìš©í•˜ì—¬ Neural Network ê¸°ë°˜ Meta-Learnerë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python src/models/train_meta_learner_mlp.py
"""

import pandas as pd
import numpy as np
import pickle
from pathlib import Path
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# ê²½ë¡œ ì„¤ì •
DATA_DIR = Path('data/processed')
MODEL_DIR = Path('models')
MODEL_DIR.mkdir(exist_ok=True)

print("=" * 60)
print("MLP Meta-Learner í•™ìŠµ ì‹œì‘")
print("=" * 60)

# ===================================================================
# STEP 1: ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# ===================================================================
print("\nğŸ“‚ OOF ë°ì´í„° ë¡œë“œ ì¤‘...")
oof_df = pd.read_csv(DATA_DIR / 'oof_predictions.csv')
print(f"âœ… Shape: {oof_df.shape}")

# Meta-Features ë° íƒ€ê²Ÿ ë¶„ë¦¬
meta_features = [
    'xgb_pred_x', 'xgb_pred_y',
    'lgb_pred_x', 'lgb_pred_y',
    'cat_pred_x', 'cat_pred_y'
]

X_meta = oof_df[meta_features].values
y_true_x = oof_df['true_x'].values
y_true_y = oof_df['true_y'].values

print(f"\nâœ… Meta-Features: {X_meta.shape}")
print(f"   - ìƒ˜í”Œ: {len(X_meta):,}ê°œ")
print(f"   - í”¼ì²˜: {len(meta_features)}ê°œ")

# ===================================================================
# STEP 2: ë°ì´í„° ì •ê·œí™” (Neural Networkìš©)
# ===================================================================
print("\nğŸ”§ ë°ì´í„° ì •ê·œí™” ì¤‘...")
scaler = StandardScaler()
X_meta_scaled = scaler.fit_transform(X_meta)

print("âœ… ì •ê·œí™” ì™„ë£Œ")

# ===================================================================
# STEP 3: PyTorch Dataset ìƒì„±
# ===================================================================
print("\nğŸ”§ PyTorch Dataset ìƒì„± ì¤‘...")

# NumPy â†’ Torch Tensor
X_tensor = torch.FloatTensor(X_meta_scaled)
y_x_tensor = torch.FloatTensor(y_true_x).unsqueeze(1)
y_y_tensor = torch.FloatTensor(y_true_y).unsqueeze(1)

# Train/Val ë¶„ë¦¬ (80:20)
from sklearn.model_selection import train_test_split

X_train, X_val, y_x_train, y_x_val = train_test_split(
    X_tensor, y_x_tensor, test_size=0.2, random_state=42
)
_, _, y_y_train, y_y_val = train_test_split(
    X_tensor, y_y_tensor, test_size=0.2, random_state=42
)

print(f"âœ… Train: {len(X_train):,}ê°œ")
print(f"âœ… Val:   {len(X_val):,}ê°œ")

# DataLoader ìƒì„±
batch_size = 256
train_dataset_x = TensorDataset(X_train, y_x_train)
train_loader_x = DataLoader(train_dataset_x, batch_size=batch_size, shuffle=True)

train_dataset_y = TensorDataset(X_train, y_y_train)
train_loader_y = DataLoader(train_dataset_y, batch_size=batch_size, shuffle=True)

# ===================================================================
# STEP 4: MLP ëª¨ë¸ ì •ì˜
# ===================================================================
print("\nğŸ—ï¸ MLP ëª¨ë¸ ì •ì˜ ì¤‘...")

class MLPMetaLearner(nn.Module):
    """
    Simple 2-layer MLP for Meta-Learning
    
    Architecture:
        Input (6) â†’ Hidden (32) â†’ ReLU â†’ Dropout â†’ Hidden (16) â†’ ReLU â†’ Output (1)
    """
    def __init__(self, input_dim=6, hidden_dim1=32, hidden_dim2=16, dropout=0.2):
        super(MLPMetaLearner, self).__init__()
        
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim1),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim1, hidden_dim2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim2, 1)
        )
    
    def forward(self, x):
        return self.network(x)

# ëª¨ë¸ ì´ˆê¸°í™”
mlp_x = MLPMetaLearner(input_dim=6)
mlp_y = MLPMetaLearner(input_dim=6)

print("âœ… MLP êµ¬ì¡°:")
print(mlp_x)

# ===================================================================
# STEP 5: í•™ìŠµ ì„¤ì •
# ===================================================================
print("\nâš™ï¸ í•™ìŠµ ì„¤ì •...")

# Loss & Optimizer
criterion = nn.MSELoss()
optimizer_x = optim.Adam(mlp_x.parameters(), lr=0.001, weight_decay=1e-4)
optimizer_y = optim.Adam(mlp_y.parameters(), lr=0.001, weight_decay=1e-4)

# Learning Rate Scheduler
scheduler_x = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer_x, mode='min', factor=0.5, patience=10
)
scheduler_y = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer_y, mode='min', factor=0.5, patience=10
)

print("âœ… Optimizer: Adam (lr=0.001)")
print("âœ… Loss: MSE")
print("âœ… Scheduler: ReduceLROnPlateau")

# ===================================================================
# STEP 6: í•™ìŠµ í•¨ìˆ˜
# ===================================================================
def train_epoch(model, loader, criterion, optimizer):
    """1 epoch í•™ìŠµ"""
    model.train()
    total_loss = 0
    
    for X_batch, y_batch in loader:
        optimizer.zero_grad()
        
        # Forward
        y_pred = model(X_batch)
        loss = criterion(y_pred, y_batch)
        
        # Backward
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item() * len(X_batch)
    
    return total_loss / len(loader.dataset)

def validate(model, X_val, y_val):
    """ê²€ì¦"""
    model.eval()
    with torch.no_grad():
        y_pred = model(X_val)
        mse = criterion(y_pred, y_val).item()
    return np.sqrt(mse)  # RMSE

# ===================================================================
# STEP 7: end_x ëª¨ë¸ í•™ìŠµ
# ===================================================================
print("\n" + "=" * 60)
print("ğŸ“ end_x MLP í•™ìŠµ ì‹œì‘")
print("=" * 60)

n_epochs = 100
best_val_rmse_x = float('inf')
patience_counter = 0
early_stop_patience = 20

for epoch in range(n_epochs):
    # Train
    train_loss = train_epoch(mlp_x, train_loader_x, criterion, optimizer_x)
    
    # Validate
    val_rmse = validate(mlp_x, X_val, y_x_val)
    
    # Scheduler step
    scheduler_x.step(val_rmse)
    
    # Early stopping
    if val_rmse < best_val_rmse_x:
        best_val_rmse_x = val_rmse
        patience_counter = 0
        # ìµœê³  ëª¨ë¸ ì €ì¥
        best_mlp_x_state = mlp_x.state_dict()
    else:
        patience_counter += 1
    
    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1:3d} | Train Loss: {train_loss:.6f} | "
              f"Val RMSE: {val_rmse:.4f}m | Best: {best_val_rmse_x:.4f}m")
    
    if patience_counter >= early_stop_patience:
        print(f"\nâš ï¸ Early stopping at epoch {epoch+1}")
        break

# ìµœê³  ëª¨ë¸ ë³µì›
mlp_x.load_state_dict(best_mlp_x_state)
print(f"\nâœ… end_x í•™ìŠµ ì™„ë£Œ! Best Val RMSE: {best_val_rmse_x:.4f}m")

# ===================================================================
# STEP 8: end_y ëª¨ë¸ í•™ìŠµ
# ===================================================================
print("\n" + "=" * 60)
print("ğŸ“ end_y MLP í•™ìŠµ ì‹œì‘")
print("=" * 60)

best_val_rmse_y = float('inf')
patience_counter = 0

for epoch in range(n_epochs):
    # Train
    train_loss = train_epoch(mlp_y, train_loader_y, criterion, optimizer_y)
    
    # Validate
    val_rmse = validate(mlp_y, X_val, y_y_val)
    
    # Scheduler step
    scheduler_y.step(val_rmse)
    
    # Early stopping
    if val_rmse < best_val_rmse_y:
        best_val_rmse_y = val_rmse
        patience_counter = 0
        best_mlp_y_state = mlp_y.state_dict()
    else:
        patience_counter += 1
    
    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1:3d} | Train Loss: {train_loss:.6f} | "
              f"Val RMSE: {val_rmse:.4f}m | Best: {best_val_rmse_y:.4f}m")
    
    if patience_counter >= early_stop_patience:
        print(f"\nâš ï¸ Early stopping at epoch {epoch+1}")
        break

# ìµœê³  ëª¨ë¸ ë³µì›
mlp_y.load_state_dict(best_mlp_y_state)
print(f"\nâœ… end_y í•™ìŠµ ì™„ë£Œ! Best Val RMSE: {best_val_rmse_y:.4f}m")

# ===================================================================
# STEP 9: ì „ì²´ ë°ì´í„° í‰ê°€
# ===================================================================
print("\n" + "=" * 60)
print("ğŸ“Š ì „ì²´ ë°ì´í„° í‰ê°€")
print("=" * 60)

mlp_x.eval()
mlp_y.eval()

with torch.no_grad():
    X_full_tensor = torch.FloatTensor(X_meta_scaled)
    
    pred_x = mlp_x(X_full_tensor).squeeze().numpy()
    pred_y = mlp_y(X_full_tensor).squeeze().numpy()

rmse_x = np.sqrt(mean_squared_error(y_true_x, pred_x))
rmse_y = np.sqrt(mean_squared_error(y_true_y, pred_y))
rmse_total = np.sqrt((rmse_x**2 + rmse_y**2) / 2)

print(f"\nâœ… MLP Meta-Learner ì„±ëŠ¥:")
print(f"   - end_x RMSE: {rmse_x:.4f}m")
print(f"   - end_y RMSE: {rmse_y:.4f}m")
print(f"   - Total RMSE: {rmse_total:.4f}m")

# ===================================================================
# STEP 10: ëª¨ë¸ ì €ì¥
# ===================================================================
print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")

# PyTorch ëª¨ë¸ + Scalerë¥¼ í•¨ê»˜ ì €ì¥
mlp_x_package = {
    'model_state': mlp_x.state_dict(),
    'scaler': scaler,
    'architecture': {
        'input_dim': 6,
        'hidden_dim1': 32,
        'hidden_dim2': 16,
        'dropout': 0.2
    }
}

mlp_y_package = {
    'model_state': mlp_y.state_dict(),
    'scaler': scaler,
    'architecture': {
        'input_dim': 6,
        'hidden_dim1': 32,
        'hidden_dim2': 16,
        'dropout': 0.2
    }
}

mlp_path_x = MODEL_DIR / 'meta_mlp_x.pkl'
mlp_path_y = MODEL_DIR / 'meta_mlp_y.pkl'

with open(mlp_path_x, 'wb') as f:
    pickle.dump(mlp_x_package, f)
with open(mlp_path_y, 'wb') as f:
    pickle.dump(mlp_y_package, f)

print(f"âœ… ì €ì¥ ì™„ë£Œ:")
print(f"   - {mlp_path_x}")
print(f"   - {mlp_path_y}")

# ===================================================================
# STEP 11: ê¸°ì¡´ Meta-Learnerì™€ ë¹„êµ
# ===================================================================
print("\n" + "=" * 60)
print("ğŸ“Š Meta-Learner ë¹„êµ")
print("=" * 60)

# Ridge/LightGBM ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸° (train_meta_learner.py ì‹¤í–‰ í•„ìš”)
try:
    # Ridge
    with open(MODEL_DIR / 'meta_ridge_x.pkl', 'rb') as f:
        ridge_x = pickle.load(f)
    with open(MODEL_DIR / 'meta_ridge_y.pkl', 'rb') as f:
        ridge_y = pickle.load(f)
    
    ridge_pred_x = ridge_x.predict(X_meta)
    ridge_pred_y = ridge_y.predict(X_meta)
    ridge_rmse_x = np.sqrt(mean_squared_error(y_true_x, ridge_pred_x))
    ridge_rmse_y = np.sqrt(mean_squared_error(y_true_y, ridge_pred_y))
    ridge_rmse = np.sqrt((ridge_rmse_x**2 + ridge_rmse_y**2) / 2)
    
    print(f"\nRidge     : {ridge_rmse:.4f}m")
except FileNotFoundError:
    ridge_rmse = None
    print(f"\nRidge     : (ëª¨ë¸ ì—†ìŒ)")

try:
    # LightGBM
    with open(MODEL_DIR / 'meta_lgb_x.pkl', 'rb') as f:
        lgb_x = pickle.load(f)
    with open(MODEL_DIR / 'meta_lgb_y.pkl', 'rb') as f:
        lgb_y = pickle.load(f)
    
    lgb_pred_x = lgb_x.predict(X_meta)
    lgb_pred_y = lgb_y.predict(X_meta)
    lgb_rmse_x = np.sqrt(mean_squared_error(y_true_x, lgb_pred_x))
    lgb_rmse_y = np.sqrt(mean_squared_error(y_true_y, lgb_pred_y))
    lgb_rmse = np.sqrt((lgb_rmse_x**2 + lgb_rmse_y**2) / 2)
    
    print(f"LightGBM  : {lgb_rmse:.4f}m")
except FileNotFoundError:
    lgb_rmse = None
    print(f"LightGBM  : (ëª¨ë¸ ì—†ìŒ)")

print(f"MLP       : {rmse_total:.4f}m â† ìƒˆë¡œ ì¶”ê°€!")

# ìµœê³  ì„±ëŠ¥ ì°¾ê¸°
results = {
    'Ridge': ridge_rmse,
    'LightGBM': lgb_rmse,
    'MLP': rmse_total
}
results = {k: v for k, v in results.items() if v is not None}

if results:
    best_model = min(results, key=results.get)
    best_score = results[best_model]
    
    print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_model} ({best_score:.4f}m)")
    
    # MLP ê°œì„ í­ ê³„ì‚°
    if lgb_rmse is not None:
        improvement = lgb_rmse - rmse_total
        if improvement > 0:
            print(f"\nâœ¨ MLPê°€ LightGBM ëŒ€ë¹„ {improvement:.4f}m ê°œì„ ! (+{improvement/lgb_rmse*100:.2f}%)")
        else:
            print(f"\nâš ï¸ MLPê°€ LightGBM ëŒ€ë¹„ {abs(improvement):.4f}m ë‚˜ì¨ ({improvement/lgb_rmse*100:.2f}%)")

# ===================================================================
print("\n" + "=" * 60)
print("ğŸ‰ MLP Meta-Learner í•™ìŠµ ì™„ë£Œ!")
print("=" * 60)

print(f"\nâœ… ì €ì¥ëœ ëª¨ë¸:")
print(f"   - MLP: meta_mlp_x.pkl, meta_mlp_y.pkl")

print(f"\në‹¤ìŒ ë‹¨ê³„:")
print(f"1. Stacking ì˜ˆì¸¡ (MLP): python src/models/predict_stacking_mlp.py")
print(f"2. ê²°ê³¼ ë¹„êµ ë° ë¶„ì„")
